{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reproduce_monkeyA_figures.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jobellet/fast_and_rich_decoding_in_VLPFC/blob/main/reproduce_monkeyA_figures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Om3YNhcm_ki"
      },
      "source": [
        "# Welcome to the python notebook reproducing all the figures of the paper entitled:\n",
        "# \"Decoding rapidly presented visual stimuli from prefrontal ensembles without report nor post-perceptual processing\"\n",
        "\n",
        "\n",
        "If you are using colab, you can simply run all the cells by pressing Ctrl+F9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXoS8KnUCD7n",
        "outputId": "4b827c0e-e815-493d-aeb6-730f77dc7a2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from matplotlib import pyplot as plt\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from scipy import stats\n",
        "import numpy.matlib\n",
        "from skimage.measure import label\n",
        "import seaborn as sb\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['font.family'] = \"Arial\"\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
        "\n",
        "# functions for loading data\n",
        "def get_monkeyA_df():\n",
        "    try:\n",
        "        df = pd.read_pickle('monkeyA.pkl')\n",
        "    except:\n",
        "        link_to_monkeyA_data = 'https://figshare.com/ndownloader/files/27869238'\n",
        "        urllib.request.urlretrieve(link_to_monkeyA_data,'monkeyA.pkl')\n",
        "        df = pd.read_pickle('monkeyA.pkl')\n",
        "    return df\n",
        "\n",
        "# functions for PSTH\n",
        "def getSpikemat(spiketimes,mint=-.1,maxt=.4):\n",
        "    '''\n",
        "    Bin spiketimes at 1 kHz, single stimuli / trials\n",
        "    '''\n",
        "    t_total = int((maxt-mint)*1000) # total time bins\n",
        "    nch = len(spiketimes)\n",
        "    spikemat = np.zeros((nch,t_total)).astype(bool)\n",
        "    for ch in range(nch):\n",
        "        tms = np.round(spiketimes[ch]*1000).astype(int) - 1 #0-indexing\n",
        "        \n",
        "        tms = tms[(tms>=mint*1000) & (tms<maxt*1000)] - int(mint*1000)\n",
        "        if len(tms)>0:\n",
        "            spikemat[ch,tms] = 1\n",
        "   \n",
        "    return spikemat\n",
        "\n",
        "def gen_time_bin(binsize,overlap,mint=-.1,maxt=.4):\n",
        "    all_bins = []\n",
        "    for i in range(int(binsize/overlap)):\n",
        "        all_bins.append(np.arange(mint,maxt,binsize)+overlap*i)\n",
        "    time_bins = np.sort(np.concatenate(all_bins)).astype(int)\n",
        "    return(time_bins)\n",
        "\n",
        "def rate_binning(spike_times,time_bins,binsize):\n",
        "    time_bins = time_bins/1000 # convert in s\n",
        "    binsize = binsize/1000 # convert in s\n",
        "    average = np.zeros((len(spike_times),len(time_bins)))\n",
        "    for i,t in enumerate(time_bins):\n",
        "        \n",
        "        for chan in range(len(spike_times)):\n",
        "            include = (spike_times[chan]>t) & (spike_times[chan]<(t+binsize))\n",
        "            average[chan,i] = sum(include)/binsize\n",
        "    return(average)\n",
        "\n",
        "# function for cluster correction for multiple comparisons\n",
        "def cluster_perm(real_data,permutations,pval_roi_threshold,pval_threshold):\n",
        "    \n",
        "    mean_perm = np.mean(permutations,axis = 0)\n",
        "    std_perm = np.std(permutations,axis = 0)\n",
        "    tval = (real_data-mean_perm)/std_perm\n",
        "    pval = np.mean(np.matlib.repmat(real_data,permutations.shape[0],1) <= permutations,axis = 0)\n",
        "    clusters = label(pval<pval_roi_threshold)\n",
        "    sum_tvals = []\n",
        "    significant_points = np.zeros_like(real_data).astype(bool)\n",
        "    for i in range(max(clusters)):\n",
        "        sum_tvals.append(sum(tval[clusters == (i+1)]))\n",
        "    \n",
        "    max_sum_tvals_perm = np.zeros(permutations.shape[0])\n",
        "    for i in range(permutations.shape[0]):\n",
        "        real_data_perm = permutations[i,:]\n",
        "        tvalperm = (real_data_perm-mean_perm)/std_perm\n",
        "        pvalperm = np.mean(np.matlib.repmat(real_data_perm,permutations.shape[0]-1,1) <= np.delete(permutations,i,axis = 0),axis = 0)\n",
        "        clusters_perm = label(pvalperm<pval_roi_threshold)\n",
        "        sum_tvals_perm = []\n",
        "        for j in range(max(clusters_perm)):\n",
        "            sum_tvals_perm.append(sum(tvalperm[clusters_perm == (j+1)]))\n",
        "        \n",
        "        if len(sum_tvals_perm)>0:\n",
        "            max_sum_tvals_perm[i] = np.max(sum_tvals_perm)\n",
        "    for i in range(max(clusters)):\n",
        "        if np.mean(sum_tvals[i]<max_sum_tvals_perm)<pval_threshold:\n",
        "            significant_points[clusters == (i+1)] = True\n",
        "    return(significant_points)\n",
        "\n",
        "# global parameters\n",
        "nch = 96 # number of channels in the Utah array\n",
        "sf = 30000; # sampling frequency\n",
        "tmin = -100 #ms, time before each stim\n",
        "tmax = 600 #ms, time after each stim\n",
        "\n",
        "# parameters for binning and normalization\n",
        "binsize = 50 #ms\n",
        "overlap = 25 #ms\n",
        "time_bins = gen_time_bin(binsize,overlap,mint=tmin,maxt=tmax)\n",
        "df = get_monkeyA_df()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9khIpPBLCnRx"
      },
      "source": [
        "# Monkey A\n",
        "\n",
        "## Obtain normalized firing rate from spike time "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "3Hb9FpIIEDa4",
        "scrolled": true,
        "outputId": "2d9c711a-9e77-4d1e-e41c-7fbdc83fd16b"
      },
      "source": [
        "compute_this_step = True # Set to false to just dowload the precomputed firing rate\n",
        "# download data from figshare\n",
        "df = get_monkeyA_df()\n",
        "\n",
        "def get_monkeyA_firing_rate():\n",
        "    try:\n",
        "        Rb = np.load('monkeyA_PFC_raw_firing_rate.npy')\n",
        "        Rc = np.load('monkeyA_PFC_normalized_firing_rate.npy')\n",
        "        \n",
        "    except:\n",
        "        link_to_monkeyA_raw_firing_rate_PFC = 'https://figshare.com/ndownloader/files/28205367'\n",
        "        urllib.request.urlretrieve(link_to_monkeyA_raw_firing_rate_PFC,'monkeyA_PFC_raw_firing_rate.npy')\n",
        "        \n",
        "        link_to_monkeyA_nomalized_firing_rate_PFC = 'https://figshare.com/ndownloader/files/28205388'\n",
        "        urllib.request.urlretrieve(link_to_monkeyA_nomalized_firing_rate_PFC,'monkeyA_PFC_normalized_firing_rate.npy')\n",
        "        Rb = np.load('monkeyA_PFC_raw_firing_rate.npy')\n",
        "        Rc = np.load('monkeyA_PFC_normalized_firing_rate.npy')\n",
        "    \n",
        "    return Rb,Rc\n",
        "\n",
        "if compute_this_step:\n",
        "    # create PFC PSTH\n",
        "    ntr = len(df) # number of stimuli presentations\n",
        "\n",
        "    nbins = len(time_bins)\n",
        "    Rb = np.zeros((ntr,nch,nbins)) # this will store the firing rate\n",
        "    Rc = np.zeros((ntr,nch,nbins)) # this will store the normalized firing rate\n",
        "    for tr in tqdm(range(ntr)): # loop through every stimuli presentations\n",
        "        Rb[tr,:] = rate_binning(df['Spikes'].iloc[tr],time_bins,binsize)\n",
        "        \n",
        "    ## center each channel\n",
        "    for sesID in np.unique(np.array(df.sesID)):\n",
        "        for ch in range(nch):\n",
        "            Rc[df.sesID == sesID,ch,:] = (Rb[df.sesID == sesID,ch,:] - np.mean(Rb[df.sesID == sesID,ch,:])) / (np.std(Rb[df.sesID == sesID,ch,:]) + 10e-6) # center each channel for each session independentl\n",
        "    np.save('monkeyA_PFC_raw_firing_rate.npy',Rb)\n",
        "    np.save('monkeyA_PFC_normalized_firing_rate.npy',Rc)\n",
        "    \n",
        "    \n",
        "else:\n",
        "    Rb,Rc = get_monkeyA_firing_rate()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-704126746f20>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    except:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BV5593sMC6G"
      },
      "source": [
        "# Train with isolated stimuli PFC array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soPnnWC2yd3U"
      },
      "source": [
        "compute_this_step = True # Set to false to just download the precomputed predictive probabilities\n",
        "\n",
        "nstim = len(np.unique(df.StimID))\n",
        "\n",
        "t = gen_time_bin(binsize,overlap,mint=tmin,maxt=tmax)\n",
        "\n",
        "if compute_this_step:\n",
        "    K = 10 # number of training folds\n",
        "    nbins = len(t) # number of time bins\n",
        "    sessions = np.unique(np.array(df.sesID).astype(int))\n",
        "\n",
        "    # index\n",
        "    single_stim = (np.array(df.TrialID)>499)  # select only stimuli presentation being followed by an other stimulus 400 ms after the onset\n",
        "    single_stim = single_stim & np.concatenate(([False],single_stim[:-1])) # select only stimuli presentation being preceded by an other stimulus 400 ms before the onset\n",
        "    \n",
        "    print(sum(single_stim))\n",
        "\n",
        "    X = Rc[:,np.mean(Rb[:,:,0],axis = 0)>1,:] # The firing rate\n",
        "    Y = np.array(df.StimID).astype(int)\n",
        "    ntr = X.shape[0]\n",
        "    ntest = int(ntr/K)\n",
        "    shuffle = np.random.permutation(ntr)# permutation of every trial\n",
        "    Coef = np.zeros((nstim,X.shape[1],nbins,K) )# regression coefficient prealocation\n",
        "    Proba = np.zeros((ntr,nstim,nbins))# predictive probability prealocation\n",
        "    Full_proba_matrix = np.zeros((ntr,nstim,nbins,nbins)) # predictive probability prealocation\n",
        "    corresponding_proba = np.zeros((ntr,nbins)) # predictive probability for concerned item prealocation\n",
        "    for k in range(K):\n",
        "        print('Fold ',k+1,' / ',K)\n",
        "        testind = shuffle[k*ntest:(k+1)*ntest] # index of trial being tested\n",
        "        if k == K-1:\n",
        "            testind = shuffle[k*ntest:] \n",
        "        Ytest = Y[testind].astype(int)\n",
        "        trainind = np.delete(shuffle,np.arange(k*ntest,(k+1)*ntest)) # index of trial being used for training\n",
        "        trainind = trainind[single_stim[trainind]] # Keeping only long ISI \n",
        "        Xtrain = X[trainind,:]\n",
        "        Ytrain = Y[trainind]\n",
        "\n",
        "        for b in range(nbins):\n",
        "            model = LogisticRegression(fit_intercept=False,solver='lbfgs',multi_class='auto',max_iter=10000).fit(Xtrain[:,:,b],Ytrain)\n",
        "            Coef[:,:,b,k] = model.coef_\n",
        "            Proba[testind,:,b] = model.predict_proba(X[testind,:,b])\n",
        "            for b2 in range(nbins):\n",
        "                Full_proba_matrix[testind,:,b,b2] = model.predict_proba(X[testind,:,b2])\n",
        "            corresponding_proba[testind,b] = np.squeeze(Proba[testind,Ytest,b])\n",
        "    np.save('monkeyA_PFC_trainsingle_Coef.npy',Coef)# regression coefficient prealocation\n",
        "    np.save('monkeyA_PFC_trainsingle_Proba.npy',Proba) # predictive probability prealocation\n",
        "    np.save('monkeyA_PFC_trainsingle_Full_proba_matrix.npy',Full_proba_matrix) # predictive probability prealocation\n",
        "    np.save('monkeyA_PFC_trainsingle_corresponding_proba.npy',corresponding_proba) # predictive probability for concerned item prealocation\n",
        "else:\n",
        "    try:\n",
        "        Coef = np.load('monkeyA_PFC_trainsingle_Coef.npy')# regression coefficient prealocation\n",
        "        Proba = np.load('monkeyA_PFC_trainsingle_Proba.npy') # predictive probability prealocation\n",
        "        Full_proba_matrix = np.load('monkeyA_PFC_trainsingle_Full_proba_matrix.npy') # predictive probability prealocation\n",
        "        corresponding_proba = np.load('monkeyA_PFC_trainsingle_corresponding_proba.npy') # predictive probability for concerned item prealocation\n",
        "    except:\n",
        "        monkeyA_PFC_trainsingle_Coef = https://figshare.com/ndownloader/files/28216620'\n",
        "        urllib.request.urlretrieve(monkeyA_PFC_trainsingle_Coef,'monkeyA_PFC_trainsingle_Coef.npy')\n",
        "        monkeyA_PFC_trainsingle_Proba = 'https://figshare.com/ndownloader/files/28216623'\n",
        "        urllib.request.urlretrieve(monkeyA_PFC_trainsingle_Proba,'monkeyA_PFC_trainsingle_Proba.npy')\n",
        "        monkeyA_PFC_trainsingle_Full_proba_matrix = 'https://figshare.com/ndownloader/files/28216650'\n",
        "        urllib.request.urlretrieve(monkeyA_PFC_trainsingle_Full_proba_matrix,'monkeyA_PFC_trainsingle_Full_proba_matrix.npy')\n",
        "        monkeyA_PFC_trainsingle_corresponding_proba = 'https://figshare.com/ndownloader/files/28216653'\n",
        "        urllib.request.urlretrieve(monkeyA_PFC_trainsingle_corresponding_proba,'monkeyA_PFC_trainsingle_corresponding_proba.npy')\n",
        "\n",
        "        Coef = np.load('monkeyA_PFC_trainsingle_Coef.npy')# regression coefficient prealocation\n",
        "        Proba = np.load('monkeyA_PFC_trainsingle_Proba.npy') # predictive probability prealocation\n",
        "        Full_proba_matrix = np.load('monkeyA_PFC_trainsingle_Full_proba_matrix.npy') # predictive probability prealocation\n",
        "        corresponding_proba = np.load('monkeyA_PFC_trainsingle_corresponding_proba.npy') # predictive probability for concerned item prealocation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZCKjw6PMC6G"
      },
      "source": [
        "## Figure 2 A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVUWAhZPMC6H"
      },
      "source": [
        "t = gen_time_bin(binsize,overlap,mint=tmin,maxt=tmax)+50 # the plotted time bin is upper end of each interval\n",
        "nperm = 2000\n",
        "stimID = np.unique(df.StimID).astype(int)\n",
        "single_stim = np.array(df.TrialID)>500 # Logical array locating stimuli presented in isolation\n",
        "single_stim = single_stim & np.concatenate(([False],single_stim[:-1])) & (df.ItemID!=5)\n",
        "\n",
        "surrogate_accuracy_RSVP = np.zeros((len(stimID),nperm,len(t)))\n",
        "# create a surrogate accuracy traces from permuted label\n",
        "# in stimuli presented inside a RSVP sequence\n",
        "for n in tqdm(stimID): # independently for each stimulus class\n",
        "    for i in range(nperm):   \n",
        "        temp = Proba[(df.TrialID == 100)& (df.ItemID>1) & (df.ItemID<5) & (df.StimID == n),:]\n",
        "        for j in range(temp.shape[0]):\n",
        "            randperm = np.random.permutation(len(stimID))\n",
        "            temp[j,:] = temp[j,randperm,:]\n",
        "        temp2 = np.argmax(temp,axis = 1) == n\n",
        "        surrogate_accuracy_RSVP[n,i,:] = np.mean(temp2, axis = 0)\n",
        "\n",
        "\n",
        "surrogate_accuracy_single_stim = np.zeros((len(stimID),nperm,len(t)))\n",
        "\n",
        "for n in tqdm(stimID):\n",
        "    for i in range(nperm):   \n",
        "        temp = Proba[single_stim & (df.StimID == n),:]\n",
        "        for j in range(temp.shape[0]):\n",
        "            randperm = np.random.permutation(len(stimID))\n",
        "            temp[j,:] = temp[j,randperm,:]\n",
        "        temp2 = np.argmax(temp,axis = 1) == n\n",
        "        surrogate_accuracy_single_stim[n,i,:] = np.mean(temp2, axis = 0)\n",
        "np.save('monkeyA_fake_hundred_mean_single_stim',surrogate_accuracy_single_stim)\n",
        "\n",
        "\n",
        "\n",
        "## cluster correction    \n",
        "\n",
        "significant_points_RSVP = np.zeros((len(stimID),len(t))).astype(bool)\n",
        "for n in tqdm(stimID):\n",
        "    # trained on hundreds\n",
        "    hundreds = np.argmax(Proba[(df.TrialID == 100)& (df.ItemID>1) & (df.ItemID<5) & (df.StimID == n),:],axis = 1) == n\n",
        "    real_data = np.mean(hundreds, axis = 0)\n",
        "    pval_roi_threshold = 0.01\n",
        "    pval_threshold = 0.001 \n",
        "    significant_points_RSVP[n,:] = cluster_perm(real_data,surrogate_accuracy_RSVP[n,:],pval_roi_threshold,pval_threshold)\n",
        "\n",
        "significant_points_single_stim = np.zeros((len(stimID),len(t))).astype(bool)\n",
        "\n",
        "for n in tqdm(stimID):\n",
        "    # trained on hundreds\n",
        "    hundreds = np.argmax(Proba[single_stim & (df.StimID == n),:],axis = 1) == n\n",
        "    real_data = np.mean(hundreds, axis = 0)\n",
        "    pval_roi_threshold = 0.01\n",
        "    pval_threshold = 0.001 \n",
        "    significant_points_single_stim[n,:] = cluster_perm(real_data,surrogate_accuracy_single_stim[n,:],pval_roi_threshold,pval_threshold)  \n",
        "\n",
        "## Draw the figure\n",
        "lightblue = [0.3,0.6,0.9]\n",
        "lightblack = [0.1,0.1,0.1]\n",
        "\n",
        "\n",
        "fig,ax = plt.subplots(int(len(np.unique(df.StimID))/3),3,figsize = (9,7), sharey = False,sharex=True)\n",
        "for n in np.unique(df.StimID).astype(int):\n",
        "    # in RSVP\n",
        "    hundreds = np.argmax(Proba[(df.TrialID == 100) & (df.ItemID<5) & (df.ItemID>1) & (df.StimID == n),:],axis = 1) == n\n",
        "    ax[np.mod(n,6),n//6].plot(t,np.mean(hundreds, axis = 0), color = lightblue,linewidth=.5,label = 'in RSVP')\n",
        "    cluster = label(significant_points_RSVP[n,:])\n",
        "    for k in np.arange(1,max(cluster)+1):\n",
        "        ax[np.mod(n,6),n//6].plot(t[cluster==k],np.mean(hundreds[:,cluster==k], axis = 0), color = lightblue,linewidth = 2)\n",
        "\n",
        "    # in single stim\n",
        "    hundreds = np.argmax(Proba[ single_stim & (df.StimID == n),:],axis = 1) == n\n",
        "    ax[np.mod(n,6),n//6].plot(t,np.mean(hundreds, axis = 0), color = lightblack,linewidth=.5,label = 'isolates stim')\n",
        "    cluster = label(significant_points_single_stim[n,:])\n",
        "    for k in np.arange(1,max(cluster)+1):\n",
        "        ax[np.mod(n,6),n//6].plot(t[cluster==k],np.mean(hundreds[:,cluster==k], axis = 0), color = lightblack,linewidth = 2)\n",
        "    _,maxval = ax[np.mod(n,6),n//6].get_ylim()\n",
        "    ax[np.mod(n,6),n//6].fill_between([0,16],[0,0],[maxval,maxval],color = 'k',alpha = 0.3)\n",
        "    ax[np.mod(n,6),n//6].plot([t[0],t[-1]],[1/18,1/18],'--',color = [.5,.5,.5],linewidth = 1)\n",
        "ax[-1,0].set_xticks([0,200,400,600])\n",
        "ax[-1,0].set_xlabel('Time from stimulus onset (ms)')\n",
        "sb.despine()\n",
        "sb.despine(trim = True)\n",
        "plt.tight_layout()\n",
        "fig.savefig('Figure2.pdf')\n",
        "fig.savefig('Figure2.jpg',dpi = 600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSKKalZ18UcJ"
      },
      "source": [
        "##Figure 3 A and B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilgXTqE5MC6I"
      },
      "source": [
        "t = gen_time_bin(binsize,overlap,mint=tmin,maxt=tmax)+50\n",
        "coolwarm = cm.get_cmap('coolwarm')\n",
        "colors = np.linspace(0,1,5)\n",
        "colors = coolwarm(colors)\n",
        "\n",
        "lightblue = [0.3,0.6,0.9]\n",
        "lightblack = [0.1,0.1,0.1]\n",
        "single_stim = np.array(df.TrialID)>500 # Logical array locating stimuli presented in isolation\n",
        "single_stim = single_stim & np.concatenate(([False],single_stim[:-1]))\n",
        "\n",
        "stims = np.unique(df.StimID).astype(int)\n",
        "all_RSVP = np.zeros((len(stims),len(t)))\n",
        "all_single = np.zeros((len(stims),len(t)))\n",
        "\n",
        "fig,ax = plt.subplots(2,1,figsize = (7,8))\n",
        "\n",
        "for it in range(5): # Loop through each position in the RSVP and single stimulus position to get average accuracy\n",
        "    for n in stims: # Loop through each stimulus class\n",
        "        # in RSVP\n",
        "        if it == 4:\n",
        "            hundreds = np.argmax(Proba[(df.ItemID == (it+1)) & (df.StimID == n),:],axis = 1) == n\n",
        "        else:\n",
        "            hundreds = np.argmax(Proba[ (df.TrialID == 100) & (df.ItemID == (it+1)) & (df.StimID == n),:],axis = 1) == n\n",
        "        all_RSVP[n,:] = np.mean(hundreds, axis = 0)\n",
        "\n",
        "        # in isolation\n",
        "        if it == 0:\n",
        "            # in single stim\n",
        "            hundreds = np.argmax(Proba[ single_stim & (df.StimID == n),:],axis = 1) == n\n",
        "            all_single[n,:] = np.mean(hundreds, axis = 0)\n",
        "\n",
        "    all_RSVP_mean = np.mean(all_RSVP, axis = 0)\n",
        "    all_RSVP_std = np.std(all_RSVP, axis = 0)\n",
        "    high_bound = all_RSVP_mean+all_RSVP_std/np.sqrt(all_RSVP.shape[0])\n",
        "    low_bound = all_RSVP_mean-all_RSVP_std/np.sqrt(all_RSVP.shape[0])\n",
        "\n",
        "    ax[0].fill_between(t+it*100,high_bound,low_bound,color = colors[it],alpha = 0.5)\n",
        "    ax[0].plot(t+it*100,np.mean(all_RSVP,axis = 0),color = colors[it]) \n",
        "    # VS chance stat\n",
        "    corrected_stats = multipletests([stats.ttest_1samp(all_RSVP[:,x],1/18)[1] for x in range(len(t))], alpha=0.001, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
        "    clusters = label(corrected_stats[0])\n",
        "    #print(it)\n",
        "    for k in np.arange(1,max(clusters)+1):\n",
        "        #print('last significant time :' + str(t[clusters==k][-1]))\n",
        "        ax[1].plot(t[clusters==k],np.ones(sum(clusters==k))*it/18+1.1, color = colors[it],linewidth = 2) \n",
        "        ax[0].plot(t[clusters==k]+it*100,np.ones(sum(clusters==k))*it/18+.6, color = colors[it],linewidth = 2) \n",
        "    ax[1].fill_between(t,high_bound,low_bound,color = colors[it],alpha = 0.5)\n",
        "    ax[1].plot(t,np.mean(all_RSVP,axis = 0),color = colors[it]) \n",
        "\n",
        "    # VS single stim stat\n",
        "\n",
        "    corrected_stats = multipletests([stats.ttest_rel(all_RSVP[:,x],all_single[:,x])[1] for x in range(len(t))], alpha=0.001, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
        "    clusters = label(corrected_stats[0])\n",
        "    for k in np.arange(1,max(clusters)+1):\n",
        "        ax[1].plot(t[clusters==k],np.ones(sum(clusters==k))*it/18+.6, color = colors[it],linewidth = 2) \n",
        "    ax[1].fill_between(t,high_bound,low_bound,color = colors[it],alpha = 0.5)\n",
        "    ax[1].plot(t,np.mean(all_RSVP,axis = 0),color = colors[it]) \n",
        "    #print('150 :')\n",
        "    #print(corrected_stats[1][t ==150])\n",
        "    #print('250 :')\n",
        "    #print(corrected_stats[1][t ==250])\n",
        "    if it == 0:\n",
        "        all_single_mean = np.mean(all_single, axis = 0)\n",
        "        all_single_std = np.std(all_single, axis = 0)\n",
        "        high_bound = all_single_mean+all_single_std/np.sqrt(all_single_mean.shape[0])\n",
        "        low_bound = all_single_mean-all_single_std/np.sqrt(all_single_mean.shape[0])\n",
        "        ax[1].fill_between(t,high_bound,low_bound,color = lightblack,alpha = 0.5)\n",
        "        ax[1].plot(t,all_single_mean,color = lightblack)\n",
        "        corrected_stats = multipletests([stats.ttest_1samp(all_single[:,x],1/18)[1] for x in range(len(t))], alpha=0.001, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
        "        clusters = label(corrected_stats[0])\n",
        "        for k in np.arange(1,max(clusters)+1):\n",
        "            ax[1].plot(t[clusters==k],np.ones(sum(clusters==k))*it/18+1, color = lightblack,linewidth = 2) \n",
        "        ax[1].fill_between(t,high_bound,low_bound,color = lightblack,alpha = 0.5)\n",
        "        ax[1].plot(t,all_single_mean,color = lightblack)  \n",
        "\n",
        "\n",
        "    ax[0].fill_between(np.array([0,16])+it*100,[0,0],[1/36,1/36],color = colors[it])\n",
        "ax[0].plot([t[0],t[-1]+it*100],[1/18,1/18],'--',color = [.5,.5,.5],linewidth = 1)\n",
        "ax[1].plot([t[0],t[-1]],[1/18,1/18],'--',color = [.5,.5,.5],linewidth = 1)\n",
        "\n",
        "\n",
        "    \n",
        "ax[0].set_xlabel('Time from first stimulus onset (ms)')\n",
        "ax[1].set_xticks([0,100,200,300,400,500,600])\n",
        "ax[1].set_xlabel('Time from stimulus onset (ms)')\n",
        "sb.despine()\n",
        "sb.despine(trim = True)\n",
        "plt.tight_layout()\n",
        "fig.savefig('Figure3.pdf')\n",
        "fig.savefig('Figure3.jpg',dpi = 600)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUHl9mKSMC6J"
      },
      "source": [
        "## Figure 4a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKfeI2qYMC6J"
      },
      "source": [
        "# Generalization accuracy matrix\n",
        "target = np.array(df.StimID).astype(int)[single_stim]\n",
        "target = np.repeat(target[:,np.newaxis],28,axis = 1)\n",
        "target = np.repeat(target[:,:,np.newaxis],28,axis = 2)\n",
        "\n",
        "Full_proba_matrix_single_stim = Full_proba_matrix[single_stim,:]\n",
        "generalization_accuracy = np.mean(np.argmax(Full_proba_matrix_single_stim,axis = 1)==target,axis = 0)\n",
        "nperm = 2000\n",
        "perm_generalization_accuracy = np.zeros((len(t),len(t),nperm))\n",
        "print('Computing surrogate generalization matrices')\n",
        "for i in tqdm(range(nperm)):\n",
        "    randperm = np.random.permutation(len(target))\n",
        "    temp_target = target[randperm,:]\n",
        "    perm_generalization_accuracy[:,:,i] = np.mean(np.argmax(Full_proba_matrix_single_stim,axis = 1)==temp_target,axis = 0)\n",
        "pvals = np.mean(np.repeat(generalization_accuracy[:,:,np.newaxis],nperm,axis = 2)<=perm_generalization_accuracy,axis = 2)\n",
        "corrected_stats = multipletests(pvals.flatten(), alpha=0.001, method='fdr_bh', is_sorted=False, returnsorted=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6XQyIJMMC6K"
      },
      "source": [
        "fig,ax = plt.subplots(1,1)\n",
        "im = ax.imshow(generalization_accuracy,origin='lower',extent=[t[0],t[-1],t[0],t[-1]],\n",
        "                 cmap = 'RdYlBu_r',vmin = 1/18,vmax = 0.5)\n",
        "\n",
        "fig.colorbar(im,ax=ax)\n",
        "\n",
        "significant_map = np.reshape(corrected_stats[0],(len(t),len(t)))\n",
        "\n",
        "ax.contour(significant_map,levels=1,extent=[t[0],t[-1],t[0],t[-1]],colors='w')\n",
        "\n",
        "ax.set_xlabel('Testing time (ms)')\n",
        "ax.set_ylabel('Training time (ms)')\n",
        "fig.savefig('Figure4A.pdf')\n",
        "fig.savefig('Figure4A.jpg',dpi = 600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS8P2HYmMC6K"
      },
      "source": [
        "## Figure 4b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOilrB4CMC6K"
      },
      "source": [
        "t = gen_time_bin(binsize,overlap,mint=tmin,maxt=tmax)+50\n",
        "significant_map = np.zeros((len(t),len(t),5))\n",
        "generalization_accuracy = np.zeros((len(t),len(t),5))\n",
        "for j in range(5):\n",
        "    if j == 0:\n",
        "         targ = ((df.ItemID==(j+1) ) & (df.TrialID == 100))\n",
        "    else:\n",
        "        targ = (df.ItemID==(j+1) )\n",
        "    \n",
        "    target = np.array(df.StimID).astype(int)[targ]\n",
        "    target = np.repeat(target[:,np.newaxis],28,axis = 1)\n",
        "    target = np.repeat(target[:,:,np.newaxis],28,axis = 2)\n",
        "\n",
        "    Full_proba_matrix_RSVP = Full_proba_matrix[targ,:]\n",
        "    generalization_accuracy[:,:,j] = np.mean(np.argmax(Full_proba_matrix_RSVP,axis = 1)==target,axis = 0)\n",
        "    nperm = 2000\n",
        "    perm_generalization_accuracy = np.zeros((len(t),len(t),nperm))\n",
        "    for i in range(nperm):\n",
        "        randperm = np.random.permutation(len(target))\n",
        "        temp_target = target[randperm,:]\n",
        "        perm_generalization_accuracy[:,:,i] = np.mean(np.argmax(Full_proba_matrix_RSVP,axis = 1)==temp_target,axis = 0)\n",
        "    pvals = np.mean(np.repeat(generalization_accuracy[:,:,j][:,:,np.newaxis],nperm,axis = 2)<=perm_generalization_accuracy,axis = 2)\n",
        "    \n",
        "    corrected_stats = multipletests(pvals.flatten(), alpha=0.001, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
        "    significant_map[:,:,j] = np.reshape(corrected_stats[0],(len(t),len(t)))\n",
        "#corrected_pvals = corrected_pvals[1].resize(len(t),len(t))\n",
        "\n",
        "\n",
        "fig,ax = plt.subplots(1,5,figsize = (16,3),sharey = True)\n",
        "for j in range(5):\n",
        "    im = ax[j].imshow(generalization_accuracy[:,:,j],origin='lower',extent=[t[0],t[-1],t[0],t[-1]],\n",
        "                    cmap = 'RdYlBu_r',vmin = 1/18,vmax = 0.5)\n",
        "    ax[j].contour(significant_map[:,:,j],levels=1,extent=[t[0],t[-1],t[0],t[-1]],colors='w')\n",
        "    ax[j].set_title('Item %d'%(j+1))\n",
        "fig.colorbar(im,ax=ax[j])\n",
        "ax[0].set_xlabel('Testing time (ms)')\n",
        "ax[0].set_ylabel('Training time (ms)')\n",
        "sb.despine()\n",
        "plt.tight_layout()\n",
        "fig.savefig('Figure4B.pdf')\n",
        "fig.savefig('Figure4B.jpg',dpi = 600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4ybXBD676bQ"
      },
      "source": [
        "# Train with stims in the RSVP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLgkL-xV76bQ"
      },
      "source": [
        "compute_this_step = True # Set to false to just download the precomputed predictive probability of classifiers trained with RSVP stimuli\n",
        "\n",
        "nstim = len(np.unique(df.StimID))\n",
        "\n",
        "t = gen_time_bin(binsize,overlap,mint=tmin,maxt=tmax)\n",
        "\n",
        "if compute_this_step:\n",
        "    K = 10 # number of training folds\n",
        "    nbins = len(t) # number of time bins\n",
        "    sessions = np.unique(np.array(df.sesID).astype(int))\n",
        "\n",
        "    # index\n",
        "    training_stims = (np.array(df.TrialID)==100)  # select only stimuli presentation being followed by an other stimulus 100 ms after the onset\n",
        "    training_stims = training_stims & np.concatenate(([False],training_stims[:-1])) # select only stimuli presentation being preceded by an other stimulus 100 ms before the onset\n",
        "    \n",
        "    print(sum(training_stims))\n",
        "\n",
        "    X = Rc[:,np.mean(Rb[:,:,0],axis = 0)>1,:] # The firing rate\n",
        "    Y = np.array(df.StimID).astype(int)\n",
        "    ntr = X.shape[0]\n",
        "    ntest = int(ntr/K)\n",
        "    shuffle = np.random.permutation(ntr)# permutation of every trial\n",
        "    Coef = np.zeros((nstim,X.shape[1],nbins,K) )# regression coefficient prealocation\n",
        "    Proba = np.zeros((ntr,nstim,nbins))# predictive probability prealocation\n",
        "    Full_proba_matrix = np.zeros((ntr,nstim,nbins,nbins)) # predictive probability prealocation\n",
        "    corresponding_proba = np.zeros((ntr,nbins)) # predictive probability for concerned item prealocation\n",
        "    for k in range(K):\n",
        "        print('Fold ',k+1,' / ',K)\n",
        "        testind = shuffle[k*ntest:(k+1)*ntest] # index of trial being tested\n",
        "        if k == K-1:\n",
        "            testind = shuffle[k*ntest:] \n",
        "        Ytest = Y[testind].astype(int)\n",
        "        trainind = np.delete(shuffle,np.arange(k*ntest,(k+1)*ntest)) # index of trial being used for training\n",
        "        trainind = trainind[training_stims[trainind]] # Keeping only long ISI \n",
        "        Xtrain = X[trainind,:]\n",
        "        Ytrain = Y[trainind]\n",
        "\n",
        "        for b in range(nbins):\n",
        "            model = LogisticRegression(fit_intercept=False,solver='lbfgs',multi_class='auto',max_iter=10000).fit(Xtrain[:,:,b],Ytrain)\n",
        "            Coef[:,:,b,k] = model.coef_\n",
        "            Proba[testind,:,b] = model.predict_proba(X[testind,:,b])\n",
        "            for b2 in range(nbins):\n",
        "                Full_proba_matrix[testind,:,b,b2] = model.predict_proba(X[testind,:,b2])\n",
        "            corresponding_proba[testind,b] = np.squeeze(Proba[testind,Ytest,b])\n",
        "    np.save('monkeyA_PFC_trainRSVP_Coef.npy',Coef)# regression coefficient prealocation\n",
        "    np.save('monkeyA_PFC_trainRSVP_Proba.npy',Proba) # predictive probability prealocation\n",
        "    np.save('monkeyA_PFC_trainRSVP_Full_proba_matrix.npy',Full_proba_matrix) # predictive probability prealocation\n",
        "    np.save('monkeyA_PFC_trainRSVP_corresponding_proba.npy',corresponding_proba) # predictive probability for concerned item prealocation\n",
        "else:\n",
        "    \n",
        "    try:\n",
        "        Coef = np.load('monkeyA_PFC_trainRSVP_Coef.npy')# regression coefficient prealocation\n",
        "        Proba = np.load('monkeyA_PFC_trainRSVP_Proba.npy') # predictive probability prealocation\n",
        "        Full_proba_matrix = np.load('monkeyA_PFC_trainRSVP_Full_proba_matrix.npy') # predictive probability prealocation\n",
        "        corresponding_proba = np.load('monkeyA_PFC_trainRSVP_corresponding_proba.npy') # predictive probability for concerned item prealocation\n",
        "    except:\n",
        "        monkeyA_PFC_trainRSVP_Coef = 'https://figshare.com/ndownloader/files/27951414'\n",
        "        urllib.request.urlretrieve(monkeyA_PFC_trainRSVP_Coef,'monkeyA_PFC_trainRSVP_Coef.npy')\n",
        "        monkeyA_PFC_trainRSVP_Proba = 'https://figshare.com/ndownloader/files/27951411'\n",
        "        urllib.request.urlretrieve(monkeyA_PFC_trainRSVP_Proba,'monkeyA_PFC_trainRSVP_Proba.npy')\n",
        "        monkeyA_PFC_trainRSVP_Full_proba_matrix = 'https://figshare.com/ndownloader/files/27951405'\n",
        "        urllib.request.urlretrieve(monkeyA_PFC_trainRSVP_Full_proba_matrix,'monkeyA_PFC_trainRSVP_Full_proba_matrix.npy')\n",
        "        monkeyA_PFC_trainRSVP_corresponding_proba = 'https://figshare.com/ndownloader/files/27951408'\n",
        "        urllib.request.urlretrieve(monkeyA_PFC_trainRSVP_corresponding_proba,'monkeyA_PFC_trainRSVP_corresponding_proba.npy')\n",
        "\n",
        "        Coef = np.load('monkeyA_PFC_trainRSVP_Coef.npy')# regression coefficient prealocation\n",
        "        Proba = np.load('monkeyA_PFC_trainRSVP_Proba.npy') # predictive probability prealocation\n",
        "        Full_proba_matrix = np.load('monkeyA_PFC_trainRSVP_Full_proba_matrix.npy') # predictive probability prealocation\n",
        "        corresponding_proba = np.load('monkeyA_PFC_trainRSVP_corresponding_proba.npy') # predictive probability for concerned item prealocation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YGnKtFl9TVY"
      },
      "source": [
        "## Figure 2 B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv-nWDjZ76bQ"
      },
      "source": [
        "t = gen_time_bin(binsize,overlap,mint=tmin,maxt=tmax)+50 # the plotted time bin is upper end of each interval\n",
        "nperm = 2000\n",
        "stimID = np.unique(df.StimID).astype(int)\n",
        "single_stim = np.array(df.TrialID)>500 # Logical array locating stimuli presented in isolation\n",
        "single_stim = single_stim & np.concatenate(([False],single_stim[:-1])) & (df.ItemID!=5)\n",
        "\n",
        "surrogate_accuracy_RSVP = np.zeros((len(stimID),nperm,len(t)))\n",
        "# create a surrogate accuracy traces from permuted label\n",
        "# in stimuli presented inside a RSVP sequence\n",
        "for n in tqdm(stimID): # independently for each stimulus class\n",
        "    for i in range(nperm):   \n",
        "        temp = Proba[(df.TrialID == 100)& (df.ItemID>1) & (df.ItemID<5) & (df.StimID == n),:]\n",
        "        for j in range(temp.shape[0]):\n",
        "            randperm = np.random.permutation(len(stimID))\n",
        "            temp[j,:] = temp[j,randperm,:]\n",
        "        temp2 = np.argmax(temp,axis = 1) == n\n",
        "        surrogate_accuracy_RSVP[n,i,:] = np.mean(temp2, axis = 0)\n",
        "\n",
        "\n",
        "surrogate_accuracy_single_stim = np.zeros((len(stimID),nperm,len(t)))\n",
        "\n",
        "for n in tqdm(stimID):\n",
        "    for i in range(nperm):   \n",
        "        temp = Proba[single_stim & (df.StimID == n),:]\n",
        "        for j in range(temp.shape[0]):\n",
        "            randperm = np.random.permutation(len(stimID))\n",
        "            temp[j,:] = temp[j,randperm,:]\n",
        "        temp2 = np.argmax(temp,axis = 1) == n\n",
        "        surrogate_accuracy_single_stim[n,i,:] = np.mean(temp2, axis = 0)\n",
        "\n",
        "\n",
        "## cluster correction    \n",
        "\n",
        "significant_points_RSVP = np.zeros((len(stimID),len(t))).astype(bool)\n",
        "for n in tqdm(stimID):\n",
        "    # trained on hundreds\n",
        "    hundreds = np.argmax(Proba[(df.TrialID == 100)& (df.ItemID>1) & (df.ItemID<5) & (df.StimID == n),:],axis = 1) == n\n",
        "    real_data = np.mean(hundreds, axis = 0)\n",
        "    pval_roi_threshold = 0.01\n",
        "    pval_threshold = 0.001 \n",
        "    significant_points_RSVP[n,:] = cluster_perm(real_data,surrogate_accuracy_RSVP[n,:],pval_roi_threshold,pval_threshold)\n",
        "\n",
        "significant_points_single_stim = np.zeros((len(stimID),len(t))).astype(bool)\n",
        "\n",
        "for n in tqdm(stimID):\n",
        "    # trained on hundreds\n",
        "    hundreds = np.argmax(Proba[single_stim & (df.StimID == n),:],axis = 1) == n\n",
        "    real_data = np.mean(hundreds, axis = 0)\n",
        "    pval_roi_threshold = 0.01\n",
        "    pval_threshold = 0.001 \n",
        "    significant_points_single_stim[n,:] = cluster_perm(real_data,surrogate_accuracy_single_stim[n,:],pval_roi_threshold,pval_threshold)  \n",
        "\n",
        "## Draw the figure\n",
        "lightblue = [0.3,0.6,0.9]\n",
        "lightblack = [0.1,0.1,0.1]\n",
        "\n",
        "\n",
        "fig,ax = plt.subplots(int(len(np.unique(df.StimID))/3),3,figsize = (9,7), sharey = False,sharex=True)\n",
        "for n in np.unique(df.StimID).astype(int):\n",
        "    # in RSVP\n",
        "    hundreds = np.argmax(Proba[(df.TrialID == 100) & (df.ItemID<5) & (df.ItemID>1) & (df.StimID == n),:],axis = 1) == n\n",
        "    ax[np.mod(n,6),n//6].plot(t,np.mean(hundreds, axis = 0), color = lightblue,linewidth=.5,label = 'in RSVP')\n",
        "    cluster = label(significant_points_RSVP[n,:])\n",
        "    for k in np.arange(1,max(cluster)+1):\n",
        "        ax[np.mod(n,6),n//6].plot(t[cluster==k],np.mean(hundreds[:,cluster==k], axis = 0), color = lightblue,linewidth = 2)\n",
        "\n",
        "    # in single stim\n",
        "    hundreds = np.argmax(Proba[ single_stim & (df.StimID == n),:],axis = 1) == n\n",
        "    ax[np.mod(n,6),n//6].plot(t,np.mean(hundreds, axis = 0), color = lightblack,linewidth=.5,label = 'isolates stim')\n",
        "    cluster = label(significant_points_single_stim[n,:])\n",
        "    for k in np.arange(1,max(cluster)+1):\n",
        "        ax[np.mod(n,6),n//6].plot(t[cluster==k],np.mean(hundreds[:,cluster==k], axis = 0), color = lightblack,linewidth = 2)\n",
        "    _,maxval = ax[np.mod(n,6),n//6].get_ylim()\n",
        "    ax[np.mod(n,6),n//6].fill_between([0,16],[0,0],[maxval,maxval],color = 'k',alpha = 0.3)\n",
        "    ax[np.mod(n,6),n//6].plot([t[0],t[-1]],[1/18,1/18],'--',color = [.5,.5,.5],linewidth = 1)\n",
        "ax[-1,0].set_xticks([0,200,400,600])\n",
        "ax[-1,0].set_xlabel('Time from stimulus onset (ms)')\n",
        "sb.despine()\n",
        "sb.despine(trim = True)\n",
        "plt.tight_layout()\n",
        "fig.savefig('FigureS4A.pdf')\n",
        "fig.savefig('FigureS4A.jpg',dpi = 600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBIup51h8su_"
      },
      "source": [
        "## Figure 3 C and D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0EXvPkY76bR"
      },
      "source": [
        "t = gen_time_bin(binsize,overlap,mint=tmin,maxt=tmax)+50 \n",
        "coolwarm = cm.get_cmap('coolwarm')\n",
        "colors = np.linspace(0,1,5)\n",
        "colors = coolwarm(colors)\n",
        "\n",
        "lightblue = [0.3,0.6,0.9]\n",
        "lightblack = [0.1,0.1,0.1]\n",
        "single_stim = np.array(df.TrialID)>500 # Logical array locating stimuli presented in isolation\n",
        "single_stim = single_stim & np.concatenate(([False],single_stim[:-1]))\n",
        "\n",
        "stims = np.unique(df.StimID).astype(int)\n",
        "all_RSVP = np.zeros((len(stims),len(t)))\n",
        "all_single = np.zeros((len(stims),len(t)))\n",
        "\n",
        "fig,ax = plt.subplots(2,1,figsize = (7,8))\n",
        "\n",
        "for it in range(5):\n",
        "    for n in stims:\n",
        "        # in RSVP\n",
        "        if it == 4:\n",
        "            hundreds = np.argmax(Proba[(df.ItemID == (it+1)) & (df.StimID == n),:],axis = 1) == n\n",
        "        else:\n",
        "            hundreds = np.argmax(Proba[ (df.TrialID == 100) & (df.ItemID == (it+1)) & (df.StimID == n),:],axis = 1) == n\n",
        "        all_RSVP[n,:] = np.mean(hundreds, axis = 0)\n",
        "\n",
        "        if it == 0:\n",
        "            # in single stim\n",
        "            hundreds = np.argmax(Proba[ single_stim & (df.StimID == n),:],axis = 1) == n\n",
        "            all_single[n,:] = np.mean(hundreds, axis = 0)\n",
        "\n",
        "    all_RSVP_mean = np.mean(all_RSVP, axis = 0)\n",
        "    all_RSVP_std = np.std(all_RSVP, axis = 0)\n",
        "    high_bound = all_RSVP_mean+all_RSVP_std/np.sqrt(all_RSVP.shape[0])\n",
        "    low_bound = all_RSVP_mean-all_RSVP_std/np.sqrt(all_RSVP.shape[0])\n",
        "\n",
        "    ax[0].fill_between(t+it*100,high_bound,low_bound,color = colors[it],alpha = 0.5)\n",
        "    ax[0].plot(t+it*100,np.mean(all_RSVP,axis = 0),color = colors[it]) \n",
        "    # VS chance stat\n",
        "    corrected_stats = multipletests([stats.ttest_1samp(all_RSVP[:,x],1/18)[1] for x in range(len(t))], alpha=0.001, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
        "    clusters = label(corrected_stats[0])\n",
        "    #print(it)\n",
        "    for k in np.arange(1,max(clusters)+1):\n",
        "        #print('last significant time :' + str(t[clusters==k][-1]))\n",
        "        ax[1].plot(t[clusters==k],np.ones(sum(clusters==k))*it/18+1.1, color = colors[it],linewidth = 2) \n",
        "        ax[0].plot(t[clusters==k]+it*100,np.ones(sum(clusters==k))*it/18+.6, color = colors[it],linewidth = 2) \n",
        "    ax[1].fill_between(t,high_bound,low_bound,color = colors[it],alpha = 0.5)\n",
        "    ax[1].plot(t,np.mean(all_RSVP,axis = 0),color = colors[it]) \n",
        "\n",
        "    # VS single stim stat\n",
        "\n",
        "    corrected_stats = multipletests([stats.ttest_rel(all_RSVP[:,x],all_single[:,x])[1] for x in range(len(t))], alpha=0.001, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
        "    clusters = label(corrected_stats[0])\n",
        "    \n",
        "    for k in np.arange(1,max(clusters)+1):\n",
        "        \n",
        "        ax[1].plot(t[clusters==k],np.ones(sum(clusters==k))*it/18+.6, color = colors[it],linewidth = 2) \n",
        "    \n",
        "    ax[1].fill_between(t,high_bound,low_bound,color = colors[it],alpha = 0.5)\n",
        "    ax[1].plot(t,np.mean(all_RSVP,axis = 0),color = colors[it]) \n",
        "    \n",
        "    #print('150 :')\n",
        "    #print(corrected_stats[1][t ==150])\n",
        "    #print('250 :')\n",
        "    #print(corrected_stats[1][t ==250])\n",
        "    if it == 0:\n",
        "        all_single_mean = np.mean(all_single, axis = 0)\n",
        "        all_single_std = np.std(all_single, axis = 0)\n",
        "        high_bound = all_single_mean+all_single_std/np.sqrt(all_single_mean.shape[0])\n",
        "        low_bound = all_single_mean-all_single_std/np.sqrt(all_single_mean.shape[0])\n",
        "        ax[1].fill_between(t,high_bound,low_bound,color = lightblack,alpha = 0.5)\n",
        "        ax[1].plot(t,all_single_mean,color = lightblack)\n",
        "        corrected_stats = multipletests([stats.ttest_1samp(all_single[:,x],1/18)[1] for x in range(len(t))], alpha=0.001, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
        "        clusters = label(corrected_stats[0])\n",
        "        for k in np.arange(1,max(clusters)+1):\n",
        "            ax[1].plot(t[clusters==k],np.ones(sum(clusters==k))*it/18+1, color = lightblack,linewidth = 2) \n",
        "        ax[1].fill_between(t,high_bound,low_bound,color = lightblack,alpha = 0.5)\n",
        "        ax[1].plot(t,all_single_mean,color = lightblack) \n",
        "\n",
        "\n",
        "    ax[0].fill_between(np.array([0,16])+it*100,[0,0],[1/36,1/36],color = colors[it])\n",
        "ax[0].plot([t[0],t[-1]+it*100],[1/18,1/18],'--',color = [.5,.5,.5],linewidth = 1)\n",
        "ax[1].plot([t[0],t[-1]],[1/18,1/18],'--',color = [.5,.5,.5],linewidth = 1)\n",
        "\n",
        "\n",
        "    \n",
        "ax[0].set_xlabel('Time from first stimulus onset (ms)')\n",
        "ax[1].set_xticks([0,100,200,300,400,500,600])\n",
        "ax[1].set_xlabel('Time from stimulus onset (ms)')\n",
        "sb.despine()\n",
        "sb.despine(trim = True)\n",
        "plt.tight_layout()\n",
        "fig.savefig('Figure3CD.pdf')\n",
        "fig.savefig('Figure3CD.jpg',dpi = 600)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxCv_X2n76bS"
      },
      "source": [
        "target = np.array(df.StimID).astype(int)[single_stim]\n",
        "target = np.repeat(target[:,np.newaxis],28,axis = 1)\n",
        "target = np.repeat(target[:,:,np.newaxis],28,axis = 2)\n",
        "\n",
        "Full_proba_matrix_single_stim = Full_proba_matrix[single_stim,:]\n",
        "generalization_accuracy = np.mean(np.argmax(Full_proba_matrix_single_stim,axis = 1)==target,axis = 0)\n",
        "nperm = 2000\n",
        "perm_generalization_accuracy = np.zeros((len(t),len(t),nperm))\n",
        "for i in tqdm(range(nperm)):\n",
        "    randperm = np.random.permutation(len(target))\n",
        "    temp_target = target[randperm,:]\n",
        "    perm_generalization_accuracy[:,:,i] = np.mean(np.argmax(Full_proba_matrix_single_stim,axis = 1)==temp_target,axis = 0)\n",
        "pvals = np.mean(np.repeat(generalization_accuracy[:,:,np.newaxis],nperm,axis = 2)<=perm_generalization_accuracy,axis = 2)\n",
        "corrected_stats = multipletests(pvals.flatten(), alpha=0.001, method='fdr_bh', is_sorted=False, returnsorted=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMctoUj-76bS"
      },
      "source": [
        "## Figure 4 C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLh7GiTO76bS"
      },
      "source": [
        "fig,ax = plt.subplots(1,1)\n",
        "im = ax.imshow(generalization_accuracy,origin='lower',extent=[t[0],t[-1],t[0],t[-1]],\n",
        "                 cmap = 'RdYlBu_r',vmin = 1/18,vmax = 0.5)\n",
        "\n",
        "fig.colorbar(im,ax=ax)\n",
        "\n",
        "significant_map = np.reshape(corrected_stats[0],(len(t),len(t)))\n",
        "\n",
        "ax.contour(significant_map,levels=1,extent=[t[0],t[-1],t[0],t[-1]],colors='w')\n",
        "plt.plot([t[0],t[-1]],[t[0],t[-1]],'k')\n",
        "ax.set_xlabel('Testing time (ms)')\n",
        "ax.set_ylabel('Training time (ms)')\n",
        "fig.savefig('Figure4C.pdf')\n",
        "fig.savefig('Figure4C.jpg',dpi = 600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxXJTNH9848t"
      },
      "source": [
        "## Figure 4 D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrzH8D-f76bU"
      },
      "source": [
        "significant_map = np.zeros((len(t),len(t),5))\n",
        "generalization_accuracy = np.zeros((len(t),len(t),5))\n",
        "for j in range(5):\n",
        "    if j == 0:\n",
        "         targ = ((df.ItemID==(j+1) ) & (df.TrialID == 100))\n",
        "    else:\n",
        "        targ = (df.ItemID==(j+1) )\n",
        "    \n",
        "    target = np.array(df.StimID).astype(int)[targ]\n",
        "    target = np.repeat(target[:,np.newaxis],28,axis = 1)\n",
        "    target = np.repeat(target[:,:,np.newaxis],28,axis = 2)\n",
        "\n",
        "    Full_proba_matrix_RSVP = Full_proba_matrix[targ,:]\n",
        "    generalization_accuracy[:,:,j] = np.mean(np.argmax(Full_proba_matrix_RSVP,axis = 1)==target,axis = 0)\n",
        "    nperm = 2000\n",
        "    perm_generalization_accuracy = np.zeros((len(t),len(t),nperm))\n",
        "    for i in range(nperm):\n",
        "        randperm = np.random.permutation(len(target))\n",
        "        temp_target = target[randperm,:]\n",
        "        perm_generalization_accuracy[:,:,i] = np.mean(np.argmax(Full_proba_matrix_RSVP,axis = 1)==temp_target,axis = 0)\n",
        "    pvals = np.mean(np.repeat(generalization_accuracy[:,:,j][:,:,np.newaxis],nperm,axis = 2)<=perm_generalization_accuracy,axis = 2)\n",
        "    \n",
        "    corrected_stats = multipletests(pvals.flatten(), alpha=0.001, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
        "    significant_map[:,:,j] = np.reshape(corrected_stats[0],(len(t),len(t)))\n",
        "#corrected_pvals = corrected_pvals[1].resize(len(t),len(t))\n",
        "\n",
        "\n",
        "fig,ax = plt.subplots(1,5,figsize = (16,3),sharey = True)\n",
        "for j in range(5):\n",
        "    im = ax[j].imshow(generalization_accuracy[:,:,j],origin='lower',extent=[t[0],t[-1],t[0],t[-1]],\n",
        "                    cmap = 'RdYlBu_r',vmin = 1/18,vmax = 0.5)\n",
        "    ax[j].contour(significant_map[:,:,j],levels=1,extent=[t[0],t[-1],t[0],t[-1]],colors='w')\n",
        "    ax[j].set_title('Item %d'%(j+1))\n",
        "fig.colorbar(im,ax=ax[j])\n",
        "ax[0].set_xlabel('Testing time (ms)')\n",
        "ax[0].set_ylabel('Training time (ms)')\n",
        "sb.despine()\n",
        "plt.tight_layout()\n",
        "fig.savefig('Figure4D.pdf')\n",
        "fig.savefig('Figure4D.jpg',dpi = 600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk3bQZK176bV"
      },
      "source": [
        "# Dropping channels out from analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2MCw6F776bV"
      },
      "source": [
        "recompute_this_step = True # Set to false to just download the precomputed predictive probabilities\n",
        "\n",
        "stims = np.unique(df.StimID).astype(int)\n",
        "nstim = len(stims)\n",
        "\n",
        "t = gen_time_bin(binsize,overlap,mint=tmin,maxt=tmax)+50\n",
        "tested_times = [75,150,300]\n",
        "tested_prop = [.05,.1,.15,.2,.25,.3,.35,.4,.45,.5,.55,.6,.65,.7,.75,.8,.85,.9,.95,1]\n",
        "\n",
        "# Sorting channels according to the absolute value of the coefficients after training\n",
        "try:\n",
        "    Coef = np.load('monkeyA_PFC_trainsingle_Coef.npy')\n",
        "except:\n",
        "    monkeyA_PFC_trainsingle_Coef = 'https://figshare.com/ndownloader/files/28216620'\n",
        "    urllib.request.urlretrieve(monkeyA_PFC_trainsingle_Coef,'monkeyA_PFC_trainsingle_Coef.npy')\n",
        "    Coef = np.load('monkeyA_PFC_trainsingle_Coef.npy')\n",
        "\n",
        "# Sort coefficients\n",
        "abs_coef = np.mean(abs(np.mean(Coef,axis = 3)),axis = 0)\n",
        "arg_sorted_coeffs = np.argsort(abs_coef,axis = 0)\n",
        "\n",
        "\n",
        "# index\n",
        "single_stim = (np.array(df.TrialID)>499)  # select only stimuli presentation being followed by an other stimulus 400 ms after the onset\n",
        "single_stim = single_stim & np.concatenate(([False],single_stim[:-1])) # select only stimuli presentation being preceded by an other stimulus 400 ms before the onset\n",
        "\n",
        "# Prealocation of the accuracy for all conditions\n",
        "chan_dependent_accuracy_best = np.zeros((len(tested_times),len(tested_prop),nstim))\n",
        "chan_dependent_accuracy_worst = np.zeros((len(tested_times),len(tested_prop),nstim))\n",
        "Y = np.array(df.StimID).astype(int)\n",
        "\n",
        "if recompute_this_step:\n",
        "    K = 10 # number of training folds\n",
        "    nbins = len(t) # number of time bins\n",
        "    sessions = np.unique(np.array(df.sesID).astype(int))\n",
        "\n",
        "    print(sum(single_stim))\n",
        "    \n",
        "    X = Rc[:,np.mean(Rb[:,:,0],axis = 0)>1,:] # The firing rate\n",
        "    for tt,testime in tqdm(enumerate(tested_times)):\n",
        "        best_chans = np.squeeze(arg_sorted_coeffs[:,np.where(t == testime)[0]])\n",
        "        worst_chans = np.flip(best_chans)\n",
        "        for tp,testprop in enumerate(tested_prop):\n",
        "            \n",
        "            Xbest = X[:,best_chans[:np.round(testprop * len(best_chans)).astype(int)],:]\n",
        "            Xworst = X[:,worst_chans[:np.round(testprop * len(worst_chans)).astype(int)],:]\n",
        "            Xbest = np.squeeze(Xbest[:,:,np.where(t == testime)[0]])\n",
        "            Xworst = np.squeeze(Xworst[:,:,np.where(t == testime)[0]])\n",
        "            \n",
        "            ntr = X.shape[0]\n",
        "            ntest = int(ntr/K)\n",
        "            shuffle = np.random.permutation(ntr)# permutation of every trial\n",
        "            \n",
        "            Probabest = np.zeros((ntr,nstim))# predictive probability prealocation\n",
        "            Probaworst = np.zeros((ntr,nstim))# predictive probability prealocation\n",
        "            for k in range(K):\n",
        "                testind = shuffle[k*ntest:(k+1)*ntest] # index of trial being tested\n",
        "                if k == K-1:\n",
        "                    testind = shuffle[k*ntest:] \n",
        "                Ytest = Y[testind].astype(int)\n",
        "                trainind = np.delete(shuffle,np.arange(k*ntest,(k+1)*ntest)) # index of trial being used for training\n",
        "                trainind = trainind[single_stim[trainind]] # Keeping only long ISI \n",
        "                Ytrain = Y[trainind]\n",
        "                \n",
        "                Xtrain = Xbest[trainind,:]\n",
        "                model = LogisticRegression(fit_intercept=False,solver='lbfgs',multi_class='auto',max_iter=10000).fit(Xtrain,Ytrain)\n",
        "                Probabest[testind,:] = model.predict_proba(Xbest[testind,:])\n",
        "                \n",
        "                Xtrain = Xworst[trainind,:]\n",
        "                model = LogisticRegression(fit_intercept=False,solver='lbfgs',multi_class='auto',max_iter=10000).fit(Xtrain,Ytrain)\n",
        "                Probaworst[testind,:] = model.predict_proba(Xworst[testind,:])\n",
        "            accuraciesbest = np.zeros(nstim)\n",
        "            accuraciesworst = np.zeros(nstim)\n",
        "            for n in stims:\n",
        "                accuraciesbest[n] = np.mean(np.argmax(Probabest[single_stim & (df.StimID == n),:],axis = 1) == n)\n",
        "                accuraciesworst[n] = np.mean(np.argmax(Probaworst[single_stim & (df.StimID == n),:],axis = 1) == n)\n",
        "            chan_dependent_accuracy_best[tt,tp,:]= accuraciesbest\n",
        "            chan_dependent_accuracy_worst[tt,tp,:]= accuraciesworst\n",
        "    np.save('chan_dependent_accuracy_best.npy',chan_dependent_accuracy_best)\n",
        "    np.save('chan_dependent_accuracy_worst.npy',chan_dependent_accuracy_worst)    \n",
        "else:\n",
        "    chan_dependent_accuracy_best = np.load('chan_dependent_accuracy_best.npy')\n",
        "    chan_dependent_accuracy_worst = np.load('chan_dependent_accuracy_worst.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa8trqaV8NOj"
      },
      "source": [
        "## Figure S6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIvX59-G76bV"
      },
      "source": [
        "tested_prop = np.array(tested_prop)\n",
        "fig,ax = plt.subplots(1,chan_dependent_accuracy_best.shape[0],figsize = (6,3),sharex = True,sharey = True)\n",
        "for i in range(chan_dependent_accuracy_best.shape[0]):\n",
        "    high_bound = np.mean(chan_dependent_accuracy_best[i,:],axis = 1)+np.std(chan_dependent_accuracy_best[i,:],axis = 1)/np.sqrt(nstim)\n",
        "    low_bound = np.mean(chan_dependent_accuracy_best[i,:],axis = 1)-np.std(chan_dependent_accuracy_best[i,:],axis = 1)/np.sqrt(nstim)\n",
        "    ax[i].fill_between(tested_prop,high_bound,low_bound,color = 'r',alpha = 0.5)\n",
        "    ax[i].plot(tested_prop,np.mean(chan_dependent_accuracy_best[i,:],axis = 1),'r')\n",
        "    corrected_stats = multipletests([stats.ttest_1samp(chan_dependent_accuracy_best[i,x,:],1/18)[1] for x in range(len(tested_prop))], alpha=0.01, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
        "    clusters = label(corrected_stats[0])\n",
        "    for k in np.arange(1,max(clusters)+1):\n",
        "        ax[i].plot(tested_prop[clusters==k],np.ones(sum(clusters==k))*.6, color = 'r',linewidth = 2) \n",
        "    print('At '+str(tested_times[i])+ ' ms,'+\n",
        "          'the ' +str(np.round(100*tested_prop[np.where(corrected_stats[0])[0][0]]).astype(int))+'% worst channels are necessary for above chance decoding')\n",
        "    \n",
        "    high_bound = np.mean(chan_dependent_accuracy_worst[i,:],axis = 1)+np.std(chan_dependent_accuracy_worst[i,:],axis = 1)/np.sqrt(nstim)\n",
        "    low_bound = np.mean(chan_dependent_accuracy_worst[i,:],axis = 1)-np.std(chan_dependent_accuracy_worst[i,:],axis = 1)/np.sqrt(nstim)\n",
        "    ax[i].fill_between(tested_prop,high_bound,low_bound,color = 'g',alpha = 0.5)\n",
        "    ax[i].plot(tested_prop,np.mean(chan_dependent_accuracy_worst[i,:],axis = 1),'g')\n",
        "    corrected_stats = multipletests([stats.ttest_1samp(chan_dependent_accuracy_worst[i,x,:],1/18)[1] for x in range(len(tested_prop))], alpha=0.01, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
        "    clusters = label(corrected_stats[0])\n",
        "    for k in np.arange(1,max(clusters)+1):\n",
        "        ax[i].plot(tested_prop[clusters==k],np.ones(sum(clusters==k))*.7, color = 'g',linewidth = 2)\n",
        "    print('At '+str(tested_times[i])+ ' ms,'+\n",
        "          'the ' +str(np.round(100*tested_prop[np.where(corrected_stats[0])[0][0]]).astype(int))+'% best channels are sufficient for above chance decoding')\n",
        "    \n",
        "    \n",
        "    ax[i].plot(tested_prop,np.ones_like(tested_prop)*1/18,'--k')\n",
        "    ax[i].set_title('t = ' + str(tested_times[i]) + ' ms')\n",
        "ax[0].set_xlabel('Proportion of channel kept')   \n",
        "ax[0].set_ylabel('Accuracy')   \n",
        "sb.despine()\n",
        "plt.tight_layout()\n",
        "fig.savefig('FigureS6.pdf')\n",
        "fig.savefig('FigureS6.jpg',dpi = 600)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}